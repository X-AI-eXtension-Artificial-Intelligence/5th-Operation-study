{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard\n",
    "- 머신러닝 실험을 위한 시각화 툴킷(toolkit)\n",
    "- TensorBoard를 사용하면 손실 및 정확도와 같은 측정 항목을 추적 및 시각화하는 것, 모델 그래프를 시각화하는 것, 히스토그램을 보는 것, 이미지를 출력하는 것 등이 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 왜 CBR2d를 UNet class 안에 구현하는 것?\n",
    "\n",
    "### ConvTranspose2d : 채널의 크기가 변경 + 이미지 크기도 업샘플링?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        ## 네트워크에서 반복적으로 사용되는 Conv + batchNorm + ReLU를 합쳐서 함수로 정의\n",
    "        # 커널 사이즈가 3x3인 Conv layer\n",
    "        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "            layers = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                          kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                nn.BatchNorm2d(num_features=out_channels),\n",
    "                nn.ReLU()\n",
    "                )\n",
    "            return layers \n",
    "        \n",
    "        # ---------constracting path (Encoder) -----------------\n",
    "        # output : [572, 572, 1(or 3)] -> [570, 570, 64]\n",
    "        self.enc1_1 = CBR2d(in_channels=3, out_channels=64)  #해당 이미지가 컬러일 경우 3, 64\n",
    "        # output : [570, 570, 64] -> [568, 568, 64]\n",
    "        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n",
    "\n",
    "        # pooling == downSampling(절반)\n",
    "        # output : [568, 568, 64] -> [284, 284, 64]\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # output : [284, 284, 64] -> [282, 282,128]\n",
    "        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n",
    "        # output : [282, 282,128] -> [280, 280,128]\n",
    "        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n",
    "        \n",
    "        # output : [280, 280, 128] -> [140, 140,128]\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # output : 140, 140,128]-> [138, 138, 256]\n",
    "        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n",
    "        # output :  [138, 138, 256] -> [136, 136, 256]\n",
    "        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n",
    "\n",
    "        # output :  [136, 136, 256] -> [68, 68, 256]\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        # output :  [68, 68, 256] -> [66, 66, 512]\n",
    "        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n",
    "        # output :  [66, 66, 512] -> [64, 64, 512]\n",
    "        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n",
    "\n",
    "        # output :  [64, 64, 512] -> [32, 32, 512]\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        # output :  [32, 32, 512] -> [30, 30, 1024]\n",
    "        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n",
    "        # output :  [30, 30, 1024] -> [28, 28, 1024]\n",
    "        self.enc5_2 = CBR2d(in_channels=1024, out_channels=1024)\n",
    "\n",
    "\n",
    "        # --------- expansive path (Decoder)--------------------\n",
    "        # output :  [28, 28, 1024] -> [56, 56, 512]  ??? 512로 줄어드는지? -> test해보기  -> 아닌듯..?\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        # output :  [56, 56, 512] -> [54, 54, 512]\n",
    "        self.dec1_1 = CBR2d(in_channels=1024, out_channels=512)\n",
    "        # output :  [54, 54, 512] -> [52, 52, 512]\n",
    "        self.dec1_2 = CBR2d(in_channels=512, out_channels=512)   # 512??? 1024???\n",
    "\n",
    "        # output :  [52, 52, 512] -> [104, 104, 256]\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        # output :  [104, 104, 256] -> [102, 102, 256]\n",
    "        self.dec2_1 = CBR2d(in_channels=512, out_channels=256)\n",
    "        # output :  [102, 102, 256] -> [100, 100, 256]\n",
    "        self.dec2_2 = CBR2d(in_channels=256, out_channels=256)\n",
    "\n",
    "        # output :  [100, 100, 256] -> [200, 200, 128]\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        # output :  [200, 200, 128] -> [188, 188, 128]\n",
    "        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n",
    "        # output :  [188, 188, 128] -> [186, 186, 128]\n",
    "        self.dec3_2 = CBR2d(in_channels=128, out_channels=128)\n",
    "\n",
    "        # output :  [186, 186, 128] -> [392, 392, 64]\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        # output :  [392, 392, 64] -> [390, 390, 64]\n",
    "        self.dec4_1 = CBR2d(in_channels=128, out_channels=64)\n",
    "        # output :  [390, 390, 64] -> [388, 388, 64]\n",
    "        self.dec4_2 = CBR2d(in_channels=64, out_channels=64)\n",
    "\n",
    "        # output :  [388, 388, 64] -> [388, 388, 2]\n",
    "        self.outconv = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1)  # RGB이면 3?\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1_1 = self.enc1_1(x)\n",
    "        e1_2 = self.enc1_2(e1_1)\n",
    "        e1_p = self.pool1(e1_2)\n",
    "\n",
    "        e2_1 = self.enc2_1(e1_p)\n",
    "        e2_2 = self.enc2_2(e2_1)\n",
    "        e2_p = self.pool2(e2_2)\n",
    "\n",
    "        e3_1 = self.enc3_1(e2_p)\n",
    "        e3_2 = self.enc3_2(e3_1)\n",
    "        e3_p = self.pool3(e3_2)\n",
    "\n",
    "        e4_1 = self.enc4_1(e3_p)\n",
    "        e4_2 = self.enc4_2(e4_1)\n",
    "        e4_p = self.pool4(e4_2)\n",
    "\n",
    "        e5_1 = self.enc5_1(e4_p)\n",
    "        e5_2 = self.enc5_2(e5_1)\n",
    "\n",
    "        # Decoder\n",
    "        d1_up = self.upconv1(e5_2)\n",
    "        #### skip connection\n",
    "        d1_c = torch.cat([d1_up,e4_2], dim=1)\n",
    "        d1_1 = self.dec1_1(d1_c)\n",
    "        d1_2 = self.dec1_2(d1_1)\n",
    "\n",
    "        d2_up = self.upconv2(d1_2)\n",
    "        d2_c = torch.cat([d2_up, e3_2], dim=1)\n",
    "        d2_1 = self.dec2_1(d2_c)\n",
    "        d2_2 = self.dec2_2(d2_1)\n",
    "\n",
    "        d3_up = self.upconv3(d2_2)\n",
    "        d3_c = torch.cat([d3_up, e2_2], dim=1)\n",
    "        d3_1 = self.dec3_1(d3_c)\n",
    "        d3_2 = self.dec3_2(d3_1)\n",
    "\n",
    "        d4_up = self.upconv4(d3_2)\n",
    "        d4_c = torch.cat([d4_up, e1_2], dim=1)\n",
    "        d4_1 = self.dec4_1(d4_c)\n",
    "        d4_2 = self.dec4_2(d4_1)\n",
    "\n",
    "        out = self.outconv(d4_2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/drive-train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/drive-train loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/drive-test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/drive-test loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "import deeplake\n",
    "train_dset = deeplake.load(\"hub://activeloop/drive-train\")\n",
    "train_loader = train_dset.pytorch(num_workers=0, batch_size=4, shuffle=True)\n",
    "\n",
    "test_dset = deeplake.load(\"hub://activeloop/drive-test\")\n",
    "test_loader = test_dset.pytorch(num_workers=0, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/drive-train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/drive-train loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/drive-test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/drive-test loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    }
   ],
   "source": [
    "## train\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "# import deeplake\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "## 사용하고 싶은데 못함\n",
    "# transform = transforms.Compose([transforms.ToTensor(),\n",
    "#                                 transforms.Normalize(mean=[0.5], std=[0.5]), \n",
    "#                                 transforms.Resize((32,32))]) \n",
    "# # 각 픽셀의 RGB 픽셀 범위는 0~255임. 이를 각각 Normalize해주는 것\n",
    "# # transforms.Normalize(mean=[0.5], std=[0.5])]) ==> 색상이 표준화됨\n",
    "\n",
    "# train_dset = deeplake.load(\"hub://activeloop/drive-train\")\n",
    "# train_loader = train_dset.pytorch(batch_size=4, shuffle=True, transform=transform)\n",
    "\n",
    "# test_dset = deeplake.load(\"hub://activeloop/drive-test\")\n",
    "# test_loader = test_dset.pytorch(batch_size=4, shuffle=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=[0.5], std=[0.5])]) \n",
    "# 각 픽셀의 RGB 픽셀 범위는 0~255임. 이를 각각 Normalize해주는 것\n",
    "# transforms.Normalize(mean=[0.5], std=[0.5])]) ==> 색상이 표준화됨\n",
    "\n",
    "train_dset = torchvision.datasets.ImageFolder(root='C:/Users/SAMSUNG/Desktop/X-AI/코드구현/data/cats/train/resized(32)', transform=transform)\n",
    "test_dset = torchvision.datasets.ImageFolder(root='C:/Users/SAMSUNG/Desktop/X-AI/코드구현/data/cats/test/resized(32)', transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dset, batch_size= 32, shuffle=True, drop_last= False)\n",
    "test_loader = DataLoader(dataset=test_dset, batch_size=32, shuffle=True, drop_last=False)\n",
    "\n",
    "num_class = len(train_dset.classes)\n",
    "# img size = 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(train_dset.classes);num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : tensor(7.6543, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please wait, filling up the shuffle buffer with samples.:   0%|          | 0.00/1.91G [1:09:16<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "model = UNet()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-06)\n",
    "\n",
    "\n",
    "loss_arr = []\n",
    "epoch = 5\n",
    "model.train()\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j, [image, label] in enumerate(train_loader):\n",
    "        x = image\n",
    "        y = label\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)\n",
    "\n",
    "        loss = loss_func(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if j % 10 ==0:\n",
    "            print(\"loss :\", loss)\n",
    "            loss_arr.append(loss.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model, 'UNet_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# 인퍼런스 모드 :  no_grad \n",
    "with torch.no_grad():\n",
    "    # 테스트로더에서 이미지와 라벨 불러와서\n",
    "    for image,label in test_loader:\n",
    "        x = image\n",
    "        y= label\n",
    "\n",
    "        # 모델에 데이터 넣고 결과값 얻기\n",
    "        output = model.forward(x)\n",
    "        _,output_index = torch.max(output,1)\n",
    "\n",
    "        \n",
    "        # 전체 개수 += 라벨의 개수\n",
    "        total += label.size(0)\n",
    "        correct += (output_index == y).sum().float()\n",
    "    \n",
    "    # 정확도 도출\n",
    "    print(\"Accuracy of Test Data: {}%\".format(100*correct/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
